[
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Final Project",
    "section": "",
    "text": "Predicting how diseases like malaria spread is a big challenge, especially as climate change shifts global temperature patterns. A key part of this prediction involves understanding how temperature affects different biological traits of mosquitoes and the parasites they carry — for example, how quickly mosquitoes develop (MDR), how fast the malaria parasite matures inside them (PDR), or how often mosquitoes bite humans. These temperature-sensitive traits are used to calculate something called the basic reproduction number, or R_0, which tells us how easily a disease can spread in a population.\nThe problem is that reliable data for these traits — especially at different temperatures — is limited and often noisy. Instead of drawing a single best-fit curve through the data, the paper we are following uses a Bayesian approach with a technique called Markov Chain Monte Carlo (MCMC) to model the uncertainty. This method creates a range of possible trait curves, not just one, and gives us a clearer picture of how confident we are in the estimates."
  },
  {
    "objectID": "introduction.html#abstract",
    "href": "introduction.html#abstract",
    "title": "Final Project",
    "section": "",
    "text": "Predicting how diseases like malaria spread is a big challenge, especially as climate change shifts global temperature patterns. A key part of this prediction involves understanding how temperature affects different biological traits of mosquitoes and the parasites they carry — for example, how quickly mosquitoes develop (MDR), how fast the malaria parasite matures inside them (PDR), or how often mosquitoes bite humans. These temperature-sensitive traits are used to calculate something called the basic reproduction number, or R_0, which tells us how easily a disease can spread in a population.\nThe problem is that reliable data for these traits — especially at different temperatures — is limited and often noisy. Instead of drawing a single best-fit curve through the data, the paper we are following uses a Bayesian approach with a technique called Markov Chain Monte Carlo (MCMC) to model the uncertainty. This method creates a range of possible trait curves, not just one, and gives us a clearer picture of how confident we are in the estimates."
  },
  {
    "objectID": "introduction.html#data-collection",
    "href": "introduction.html#data-collection",
    "title": "Final Project",
    "section": "1. Data Collection",
    "text": "1. Data Collection\nThe authors assembled two types of datasets:\n\nMain data: Laboratory data under constant temperature, focused on Anopheles gambiae and Plasmodium falciparum. These data were only available for three traits mosquito development rate (MDR), egg-to-adult survival (pEA), and adult mosquito mortality (\\(\\mu\\)). For other traits ideal data, the researcher used data from related species.\nPrior data: A broader set that included similar traits from related species and less controlled experiments. This dataset was used to construct informative priors in the Bayesian models.\n\n\nPlasmodium Falciparum (parasite)\n\n\n\n\n\n\n\n\nAnopheles Gambiae (mosquitoes)"
  },
  {
    "objectID": "introduction.html#modeling-thermal-responses",
    "href": "introduction.html#modeling-thermal-responses",
    "title": "Final Project",
    "section": "2. Modeling Thermal Responses",
    "text": "2. Modeling Thermal Responses\nEach biological trait (component of the basic reproductive number, R₀) was modeled as a unimodal response to temperature, typically hump-shaped.\nTwo types of thermal performance functions were used:\n\nBrière Model (asymmetric, for traits like MDR): \\[\nf(T) = a \\cdot T \\cdot (T - T_0) \\cdot \\sqrt{T_m - T}\n\\]\nQuadratic Model (symmetric, for other traits): \\[\nf(T) = -a \\cdot (T - T_{opt})^2 + b\n\\]\n\nChoice of function was based on the biological properties of each trait."
  },
  {
    "objectID": "introduction.html#likelihoods-and-constraints",
    "href": "introduction.html#likelihoods-and-constraints",
    "title": "Final Project",
    "section": "3. Likelihoods and Constraints",
    "text": "3. Likelihoods and Constraints\n\nMost traits used a truncated normal likelihood, which ensures outputs are biologically feasible\nFor probability traits (like vector competence or survival), the authors used a binomial likelihood when raw counts were available.\nAll values were constrained within biologically meaningful ranges. For example, traits expressed as probabilities were constrained to [0, 1]."
  },
  {
    "objectID": "introduction.html#bayesian-inference-workflow",
    "href": "introduction.html#bayesian-inference-workflow",
    "title": "Final Project",
    "section": "2. Bayesian Inference Workflow",
    "text": "2. Bayesian Inference Workflow\nIn this study, Bayesian inference is used to estimate the thermal responses of individual biological traits that drive malaria transmission. The goal is to determine how key mosquito and parasite traits vary with temperature, and ultimately, how this affects the basic reproduction number R_0.\nThe Bayesian approach proceeds in three major steps:\n\n1. Define a Likelihood Function\nFor each biological trait (e.g., mosquito development rate, parasite development rate), a thermal response curve—either a Brière (asymmetric) or quadratic (symmetric) function—is chosen to describe how the trait changes with temperature. The observed data are then modeled as arising from a truncated normal distribution centered around this function, with lower truncation at zero to reflect biological constraints (e.g., traits like development rate cannot be negative). For traits that are probabilities (e.g., vector competence), a binomial likelihood is used if raw count data are available.\n\n\n2. Specify Prior Distributions\nPrior distributions are selected for the parameters of the thermal response functions, including biologically motivated temperature bounds (e.g., 0°C–45°C). Initially, uninformative priors are used to allow the data to dominate the inference. Later, informative priors are constructed by fitting the model to a broader prior dataset, and then summarizing the resulting posterior estimates to inform the main analysis. This two-step prior construction helps stabilize the inference when main data are sparse or noisy.\n\n\n3. Sample from the Posterior via MCMC\nOnce likelihood and priors are specified, inference proceeds by generating posterior samples using Markov Chain Monte Carlo (MCMC) techniques such as JAGS. These samples represent plausible combinations of parameter values given both the prior knowledge and the observed data. From these, the posterior mean curves and credible intervals (e.g., 95% HPD bands) are derived to quantify uncertainty.\n\n2.1 Likelihood Definition\nWe will use the parasite deveolpment rate (PDR) as an illustration of the general workflow. According to the paper, researchers assumed that trait values vary nonlinearly with temperature and exhibit a unimodal, asymmetric pattern. This biological response was modeled using a Brière function, a common choice for temperature-dependent traits that increase sharply, peak at an intermediate temperature, and decline at high temperatures:\n\\[f(T) = c \\cdot T \\cdot (T - T_0) \\cdot \\sqrt{T_m - T}, \\quad \\text{for } T_0 &lt; T &lt; T_m\\] Here: - T is the temperature (°C), - \\(T_0\\) is the lower developmental threshold, - \\(T_m\\) is the upper thermal limit, - c is a scaling constant.\nTo connect this deterministic curve to the observed data, we assume that each observed PDR value \\(Y_i\\) (e.g., from a lab experiment) is a noisy realization of the theoretical value \\(f(T_i)\\), where \\(T_i\\) is the temperature at which the observation was recorded. This noise is modeled using a truncated normal distribution, ensuring all simulated values remain biologically meaningful (i.e., non-negative):\n\\[Y_i \\sim \\text{Normal}\\left(f(T_i), \\sigma^2\\right) \\quad \\text{truncated at } 0\\]\n\n\n2.2 Prior Distribution\nIn the Bayesian framework, prior distributions express our beliefs about the parameters before seeing the data. For the parasite development rate (PDR), the model includes four parameters: the lower thermal limit \\(T_0\\), upper thermal limit \\(T_m\\), a scaling constant c, and a standard deviation \\(\\sigma\\) representing observation noise.\nBased on biological constraints and prior studies (as described in the paper and Appendix A.3), the following priors were chosen:\n\n\\(T_0\\) (lower bound of development): We assume development does not occur below roughly 5°C. So we used a truncated normal prior: \\(T_0 \\sim \\text{Normal}(12.3, 2.5^2) \\text{ truncated at } [0, ]\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(T_m\\) (upper limit of development): Mosquitoes and malaria parasites typically die or stop developing near 40°C. We used: \\(T_m \\sim \\text{Normal}(35, 1.0^2) \\text{ truncated at } [T_0, 45]\\)\n\n\n\n\n\n\n\n\n\n\n\nc (scaling parameter of the Brière function): The scaling constant c controls the overall height of the thermal curve. Since it must be positive and is hard to predict precisely, we used a weakly informative gamma prior: \\(c \\sim \\text{Gamma}(2, 40000)\\)\n\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nI honetly don’t know why we choose this but just based on the graph this puts most of the prior mass near zero but allows a long right tail.\n\n\\(\\sigma\\) (standard deviation of noise): We assume the variability in the observations is positive but small. So: \\(\\sigma \\sim \\text{Uniform}(0, 1)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n2.3 Posterior Sampling via MCMC\nMathematically, Bayes’ theorem tells us: \\(\\text{Posterior} \\propto \\text{Likelihood} \\times \\text{Prior}\\) But in practice, this product cannot be simplified or integrated analytically, especially when our model is nonlinear (like the Brière function for PDR) and high-dimensional.\nThis is where Markov Chain Monte Carlo (MCMC) comes in as we mentioned before.\nThe Tool: JAGS\n\nlibrary(rjags)\n\nLoading required package: coda\n\n\nLinked to JAGS 4.3.2\n\n\nLoaded modules: basemod,bugs\n\n\nTo estimate the posterior, we used JAGS (Just Another Gibbs Sampler). It allows us to simulate draws from the posterior distribution, even when the model is complex and has truncated or nonlinear components.\nIn our case, we used MCMC to sample from the joint posterior of the four parameters:\\(T_0, T_m, c, \\sigma\\)\nThe Process:\nWe ran 3 parallel chains to ensure convergence from different starting points. - Each chain ran for 5000 iterations, with a 1000-iteration burn-in period to discard the early “warm-up” samples. - In total, this gave us 12,000 posterior samples (3 chains × 4000 samples each).\nThe JAGS output gave us: - Posterior means (best estimate for each parameter) - 95% credible intervals (uncertainty) - Diagnostics like effective sample size and trace plots\nThese samples allow us to: - Estimate the PDR curve across temperature - Quantify uncertainty (via credible bands) - Propagate this uncertainty into the final R_0 calculation # Inference in Bayesian framework\nUsually proceed in three steps.\n\nA likelihood is defined for each type of data.\nAppropriate prior distributions are determined.\nSamples from the posterior distribution of hte parameters via MCMC.\n\n\nlibrary(rjags)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ stringr   1.5.1\n✔ forcats   1.0.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\n# Load data\nmosquitoes1 &lt;- readxl::read_excel(here(\"data\", \"the_one_we_need.xlsx\"))\n\nNew names:\n• `` -&gt; `...10`\n\n# Filter and prepare PDR data\npdr_data &lt;- mosquitoes1 %&gt;%\n  filter(OriginalTraitName == \"EIP (extrinsic incubation period)\",\n         !is.na(Interactor1Temp),\n         !is.na(OriginalTraitValue)) %&gt;%\n  mutate(\n    temp = as.numeric(Interactor1Temp),\n    pdr = 1 / as.numeric(OriginalTraitValue)\n  ) %&gt;%\n  arrange(temp)\n\n# Define JAGS model string\njags_model &lt;- \"\nmodel {\n  for (i in 1:N) {\n    pdr[i] ~ dnorm(mu[i], tau)\n    mu[i] &lt;- c * temp[i] * (temp[i] - T0) * sqrt(Tm - temp[i])\n  }\n\n  # Priors (informative)\n  T0 ~ dnorm(14, 1 / (3.5^2))\n  tm ~ dgamma(14.7, 3.1)\n  Tm &lt;- 31 + tm\n  c ~ dexp(100)\n\n  # Likelihood precision\n  tau &lt;- pow(sigma, -2)\n  sigma ~ dunif(0, 10)\n}\n\"\n\n# Bundle data for JAGS\ndata_jags &lt;- list(\n  temp = pdr_data$temp,\n  pdr = pdr_data$pdr,\n  N = nrow(pdr_data)\n)\n\n# Initial values\ninits &lt;- function() {\n  list(T0 = 14, tm = 5, c = 0.001, sigma = 1)\n}\n\n# Parameters to monitor\nparams &lt;- c(\"T0\", \"Tm\", \"c\", \"sigma\")\n\n# Run the JAGS model\nmodel &lt;- jags.model(textConnection(jags_model), data = data_jags, inits = inits, n.chains = 3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 8\n   Unobserved stochastic nodes: 4\n   Total graph size: 56\n\nInitializing model\n\nupdate(model, 1000)  # Burn-in\nsamples &lt;- rjags::coda.samples(model, variable.names = params, n.iter = 4000)\n\n# Summarize posterior\nsummary(samples)\n\n\nIterations = 2001:6000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 4000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n           Mean        SD  Naive SE Time-series SE\nT0    1.012e+01 2.186e+00 1.996e-02      1.024e-01\nTm    3.568e+01 1.241e+00 1.133e-02      1.806e-02\nc     5.625e-05 1.756e-05 1.603e-07      7.750e-07\nsigma 6.881e-03 2.968e-03 2.709e-05      1.021e-04\n\n2. Quantiles for each variable:\n\n           2.5%       25%       50%       75%     97.5%\nT0    5.424e+00 8.715e+00 1.026e+01 1.164e+01 1.402e+01\nTm    3.355e+01 3.481e+01 3.559e+01 3.645e+01 3.838e+01\nc     3.225e-05 4.424e-05 5.320e-05 6.435e-05 9.909e-05\nsigma 3.567e-03 4.973e-03 6.152e-03 7.905e-03 1.464e-02\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(coda)\nlibrary(tidyr)\n\n# Step 1: Extract summary statistics\nsummary_stats &lt;- summary(samples)\npost_means &lt;- summary_stats$statistics[, \"Mean\"]\nT0 &lt;- post_means[\"T0\"]\nTm &lt;- post_means[\"Tm\"]\nc_param &lt;- post_means[\"c\"]\n\n# Step 2: Define the Brière function\nbriere &lt;- function(T, T0, Tm, c) {\n  out &lt;- c * T * (T - T0) * sqrt(Tm - T)\n  out[T &lt;= T0 | T &gt;= Tm] &lt;- 0\n  return(out)\n}\n\n# Step 3: Generate prediction ribbon from posterior draws\nposterior_df &lt;- do.call(rbind, samples)\ntemps &lt;- seq(5, 45, length.out = 200)\npred_matrix &lt;- apply(posterior_df, 1, function(row) {\n  briere(temps, row[\"T0\"], row[\"Tm\"], row[\"c\"])\n})\npred_summary &lt;- data.frame(\n  temp = temps,\n  mean = rowMeans(pred_matrix),\n  lwr = apply(pred_matrix, 1, quantile, 0.025),\n  upr = apply(pred_matrix, 1, quantile, 0.975)\n)\n\n# Step 4: Clean observed data\npdr_data_clean &lt;- mosquitoes1 %&gt;%\n  filter(OriginalTraitName == \"parasite development rate (1/EIP)\") %&gt;%\n  mutate(\n    temp = as.numeric(Interactor1Temp),\n    PDR = as.numeric(OriginalTraitValue),\n    species_marker = case_when(\n      Interactor1Species == \"gambiae\" ~ \"An. gambiae\",\n      grepl(\"falciparum\", OriginalTraitDef, ignore.case = TRUE) ~ \"Other anopheline\",\n      TRUE ~ \"Other\"\n    ),\n    shape = case_when(\n      species_marker == \"An. gambiae\" ~ 19,  # solid circle\n      species_marker == \"Other anopheline\" ~ 3,  # plus\n      TRUE ~ 8  # star\n    )\n  ) %&gt;%\n  filter(!is.na(temp), !is.na(PDR))\n\n# Step 5: Plot\nggplot() +\n  geom_ribbon(data = pred_summary, aes(x = temp, ymin = lwr, ymax = upr), alpha = 0.2, linetype = \"dashed\") +\n  geom_line(data = pred_summary, aes(x = temp, y = mean), size = 1) +\n  geom_point(data = pdr_data_clean, aes(x = temp, y = PDR, shape = species_marker), size = 2) +\n  scale_shape_manual(values = c(\"An. gambiae\" = 19, \"Other anopheline\" = 3, \"Other\" = 8)) +\n  labs(\n    x = \"Temperature (°C)\",\n    y = \"Parasite development rate, PDR\",\n    shape = \"Species\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHere we finally reproduced the posterior for PDR: )"
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "Inference in Bayesian framework",
    "section": "",
    "text": "Inference in Bayesian framework\nUsually proceed in three steps.\n\nA likelihood is defined for each type of data.\nAppropriate prior distributions are determined.\nSamples from the posterior distribution of hte parameters via MCMC.\n\n\nlibrary(rjags)\n\nLoading required package: coda\n\n\nLinked to JAGS 4.3.2\n\n\nLoaded modules: basemod,bugs\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/yusiyao/Desktop/VT/STAT_4664_Stochastic/final\n\n# Load data\nmosquitoes1 &lt;- readxl::read_excel(here(\"data\", \"the_one_we_need.xlsx\"))\n\nNew names:\n• `` -&gt; `...10`\n\n# Filter and prepare PDR data\npdr_data &lt;- mosquitoes1 %&gt;%\n  filter(OriginalTraitName == \"EIP (extrinsic incubation period)\",\n         !is.na(Interactor1Temp),\n         !is.na(OriginalTraitValue)) %&gt;%\n  mutate(\n    temp = as.numeric(Interactor1Temp),\n    pdr = 1 / as.numeric(OriginalTraitValue)\n  ) %&gt;%\n  arrange(temp)\n\n# Define JAGS model string\njags_model &lt;- \"\nmodel {\n  for (i in 1:N) {\n    pdr[i] ~ dnorm(mu[i], tau)\n    mu[i] &lt;- c * temp[i] * (temp[i] - T0) * sqrt(Tm - temp[i])\n  }\n\n  # Priors (informative)\n  T0 ~ dnorm(14, 1 / (3.5^2))\n  tm ~ dgamma(14.7, 3.1)\n  Tm &lt;- 31 + tm\n  c ~ dexp(100)\n\n  # Likelihood precision\n  tau &lt;- pow(sigma, -2)\n  sigma ~ dunif(0, 10)\n}\n\"\n\n# Bundle data for JAGS\ndata_jags &lt;- list(\n  temp = pdr_data$temp,\n  pdr = pdr_data$pdr,\n  N = nrow(pdr_data)\n)\n\n# Initial values\ninits &lt;- function() {\n  list(T0 = 14, tm = 5, c = 0.001, sigma = 1)\n}\n\n# Parameters to monitor\nparams &lt;- c(\"T0\", \"Tm\", \"c\", \"sigma\")\n\n# Run the JAGS model\nmodel &lt;- jags.model(textConnection(jags_model), data = data_jags, inits = inits, n.chains = 3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 8\n   Unobserved stochastic nodes: 4\n   Total graph size: 56\n\nInitializing model\n\nupdate(model, 1000)  # Burn-in\nsamples &lt;- rjags::coda.samples(model, variable.names = params, n.iter = 4000)\n\n# Summarize posterior\nsummary(samples)\n\n\nIterations = 2001:6000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 4000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n           Mean        SD  Naive SE Time-series SE\nT0    9.884e+00 2.163e+00 1.974e-02      9.832e-02\nTm    3.569e+01 1.221e+00 1.115e-02      1.771e-02\nc     5.444e-05 1.683e-05 1.536e-07      7.167e-07\nsigma 6.703e-03 2.768e-03 2.527e-05      7.355e-05\n\n2. Quantiles for each variable:\n\n           2.5%       25%       50%       75%     97.5%\nT0    5.303e+00 8.526e+00 9.979e+00 1.138e+01 1.386e+01\nTm    3.359e+01 3.483e+01 3.558e+01 3.645e+01 3.834e+01\nc     3.171e-05 4.321e-05 5.115e-05 6.192e-05 9.699e-05\nsigma 3.499e-03 4.879e-03 6.042e-03 7.748e-03 1.361e-02\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(coda)\nlibrary(tidyr)\n\n# Step 1: Extract summary statistics\nsummary_stats &lt;- summary(samples)\npost_means &lt;- summary_stats$statistics[, \"Mean\"]\nT0 &lt;- post_means[\"T0\"]\nTm &lt;- post_means[\"Tm\"]\nc_param &lt;- post_means[\"c\"]\n\n# Step 2: Define the Brière function\nbriere &lt;- function(T, T0, Tm, c) {\n  out &lt;- suppressWarnings(c * T * (T - T0) * sqrt(Tm - T))\n  out[T &lt;= T0 | T &gt;= Tm] &lt;- 0\n  return(out)\n}\n\n# Step 3: Generate prediction ribbon from posterior draws\nposterior_df &lt;- do.call(rbind, samples)\ntemps &lt;- seq(5, 45, length.out = 200)\npred_matrix &lt;- apply(posterior_df, 1, function(row) {\n  briere(temps, row[\"T0\"], row[\"Tm\"], row[\"c\"])\n})\npred_summary &lt;- data.frame(\n  temp = temps,\n  mean = rowMeans(pred_matrix),\n  lwr = apply(pred_matrix, 1, quantile, 0.025),\n  upr = apply(pred_matrix, 1, quantile, 0.975)\n)\n\n# Step 4: Clean observed data\npdr_data_clean &lt;- mosquitoes1 %&gt;%\n  filter(OriginalTraitName == \"parasite development rate (1/EIP)\") %&gt;%\n  mutate(\n    temp = as.numeric(Interactor1Temp),\n    PDR = as.numeric(OriginalTraitValue),\n    species_marker = case_when(\n      Interactor1Species == \"gambiae\" ~ \"An. gambiae\",\n      grepl(\"falciparum\", OriginalTraitDef, ignore.case = TRUE) ~ \"Other anopheline\",\n      TRUE ~ \"Other\"\n    ),\n    shape = case_when(\n      species_marker == \"An. gambiae\" ~ 19,  # solid circle\n      species_marker == \"Other anopheline\" ~ 3,  # plus\n      TRUE ~ 8  # star\n    )\n  ) %&gt;%\n  filter(!is.na(temp), !is.na(PDR))\n\n# Step 5: Plot\nggplot() +\n  geom_ribbon(data = pred_summary, aes(x = temp, ymin = lwr, ymax = upr), alpha = 0.2, linetype = \"dashed\") +\n  geom_line(data = pred_summary, aes(x = temp, y = mean), size = 1) +\n  geom_point(data = pdr_data_clean, aes(x = temp, y = PDR, shape = species_marker), size = 2) +\n  scale_shape_manual(values = c(\"An. gambiae\" = 19, \"Other anopheline\" = 3, \"Other\" = 8)) +\n  labs(\n    x = \"Temperature (°C)\",\n    y = \"Parasite development rate, PDR\",\n    shape = \"Species\"\n  ) +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: No shared levels found between `names(values)` of the manual scale and the\ndata's shape values."
  },
  {
    "objectID": "team.html",
    "href": "team.html",
    "title": "Progress Report 1",
    "section": "",
    "text": "Yusi:\nEverything for now.\n\nProgress Report 1\n\n\n\n\nProgress Report 2"
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "What Is This Paper About?",
    "section": "",
    "text": "What Is This Paper About?\nAt its core, this paper tries to better understand how temperature affects the transmission of malaria, a disease caused by a parasite and spread by mosquitoes. More specifically, it looks at how different biological traits like, how fast mosquitoes grow or how quickly parasites develop, change depending on temperature.\nThese temperature-sensitive traits are key parts of a formula called \\(R_0\\), which is the basic reproduction number. In simple terms, \\(R_0\\) tells us how easily a disease can spread in a population. If \\(R_0\\) is high, an outbreak can happen. If it’s low, the disease dies out.\nThe most widely used formulation for malaria \\(R_0\\) (Diez 1993) defrived from the Ross-Macdonald Model (1952): \\[\nR_0 = \\sqrt{\\frac{M}{Nr} \\times \\frac{a^2 bc \\exp(-\\mu/\\text{PDR})}{\\mu}}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nParameter\nDefinition\nPrior Species\nPrior Sources\nMain Species\nMain Sources\n\n\n\n\na\nbiting rate\nAedes albopictus\nCalado and Navarro-Silva (2002), Delatte et al. (2009)\nAn. pseudopunctipennis\nLardeux et al. (2008)\n\n\nMDR\nmosquito development rate\nAn. gambiae, An. quadrimaculatus, Aedes triseriatus, Aedes aegypti, An. superpictus\nJepson et al. (1947), Love and Whlechel (1957), Jalil (1972), Joshi et al. (1996), Briegel et al. (2001), Aytekin et al. (2009)\nAn. gambiae\nBayoh and Lindsay (2003)\n\n\npₑₐ\negg to adult survival\nAn. quadrimaculatus, Aedes triseriatus, Aedes albopictus\nLove and Whlechel (1957), Jalil (1972), Delatte et al. (2009)\nAn. gambiae\nBayoh and Lindsay (2003)\n\n\nEFD\nfecundity\nAedes aegypti\nJoshi et al. (1996)\nAedes albopictus\nCalado and Navarro-Silva (2002), Delatte et al. (2009)\n\n\nμ\nmosquito mortality rate\nAedes aegypti, An. superpictus\nJoshi et al. (1996), Aytekin et al. (2009), Costantini et al. (1996), Gary & Foster (2004), Impoinvil et al. (2007), Midega et al. (2007), Okech et al. (2003)\nAn. gambiae\nBayoh (2001)\n\n\nbc\nvector competence\nP. relictum in Culex quinquefasciatus\nLaPointe et al. (2010)\nP. vivax in An. quadrimaculatus\nStratman-Thomas (1940)\n\n\nPDR\nparasite development rate\nP. vivax in An. quadrimaculatus, P. falciparum in An. gambiae\nStratman-Thomas (1940), Cambournac (1942), Boyd (1949)\nP. falciparum in An. gambiae, An. culicifacies, An. stephensi, An. quadrimaculatus, An. atroparvus\nBoyd and Stratman-Thomas (1933), Knowles and Basu (1943), Siddons (1944), Shute and Maryon (1952), Vaughan et al. (1992), Eling et al. (2001)\n\n\n\n\nNote: EIP is the extrinsic incubation period of the parasite.\n\nAnd the expected mosquito density is given by: \\[\nM = \\frac{\\text{EFD} \\cdot p_{EA} \\cdot \\text{MDR}}{\\mu^2}\n\\]\nBut to build this \\(R_0\\) model, the authors first need to estimate how these traits behave across temperatures. And that’s where the challenge begins.\n\n\nThe Big Obstacle\nOne of the biggest hurdles in this study is data scarcity. For many of the temperature-sensitive traits—like mosquito development rate (MDR) or parasite development rate (PDR)—the researchers had access to only about 8 to 12 usable data points in this example. That’s barely enough to fit a meaningful curve, especially if the data is spread out across temperatures or varies in quality.\nSo, why is the data so limited?\nExperimental constraints in lab settings. Since malaria is caused by a parasite (Plasmodium falciparum) that lives inside mosquitoes (Anopheles gambiae), studying the full transmission cycle requires infecting the mosquitoes. But under lab conditions, researchers cannot simply feed infected mosquitoes with their own blood to continue experiments, that would pose serious biohazard and ethical issues. This makes it extremely difficult to collect controlled, high-quality data for infected mosquitoes, especially across multiple temperatures.\nBeyond that, additional limitations include:\n\nEthical regulations around working with human-infectious diseases.\nTechnical difficulty of maintaining mosquitoes and parasites at constant temperatures in controlled environments.\nCost and time involved in conducting these studies, especially with biological replication.\n\nAs a result, the authors had to rely on a patchwork of data:\n\nSome traits came from direct lab studies on An. gambiae and P. falciparum.\nOthers had to be pulled from related mosquito or parasite species, or from less controlled studies, just to have enough data to work with.\n\n\n\nThe Solution: Bayesian Inference + MCMC\nTo deal with the limited and messy data, Prof.Johnson and her team used Bayesian inference, a statistical approach that is especially powerful when data is sparse or uncertain. Instead of trying to force-fit a single best curve through a handful of points, the Bayesian method treats the unknown curve parameters as random variables and estimates their entire probability distribution.\nThis is where MCMC (Markov Chain Monte Carlo) comes in. MCMC is a tool that lets you simulate samples from complicated distributions, like the posterior distribution of trait curves in this study. In simpler terms, it helps us generate many possible curves that are consistent with the data we do have—along with a sense of how confident we can be in each one.\nSo rather than returning a single line through the data, Johnson’s model gives you:\n\nA mean curve (the best guess)\nPlus a 95% credible interval (a range where the “true” curve likely falls).\n\nThis uncertainty is crucial. Because the biological data is so limited, we cannot pretend we know the exact trait-temperature relationship. But by using a Bayesian framework, the authors can at least show what’s plausible, based on both the data and prior biological knowledge.\nIn our project, we adopt this same logic. Using the real (limited) data, we apply a Bayesian curve-fitting approach to recreate key trait-temperature plots from the paper, starting with MDR and PDR."
  },
  {
    "objectID": "mcmc.html#bayesian-inference",
    "href": "mcmc.html#bayesian-inference",
    "title": "MCMC Tutorial",
    "section": "Bayesian Inference",
    "text": "Bayesian Inference\nBayesian inference provides a principled way to update our beliefs in light of data. It does so by combining a prior distribution \\(\\pi(q)\\), which encodes what we already believe, with a likelihood \\(\\pi(D \\mid q)\\), which represents how the data relate to the parameters. Together, they form the posterior:\n\\(\\pi(q \\mid D) \\propto \\pi(D \\mid q) \\, \\pi(q)\\)\nThis posterior captures everything we know about the parameters after observing data. As the speaker in the referenced talk put it:\n“These two things together with some data we get a posterior distribution and the posterior distribution captures all the information in our system. It’s a wonderfully kind of elegant theoretical property…”\nBut here’s the catch:\n“…the reality is that we can’t work with a posterior distribution. A high-dimensional probability distribution is a kind of meaningless thing. It’s this abstraction that we can’t interact with.”\nSo, how do we extract useful insights?\n\nFrom Posteriors to Expectations\nIn Bayesian inference, once we have our model and some data, the goal is to compute the posterior distribution, which combines our prior beliefs and the information from the data. This is usually written as:\n\\(\\pi(q \\mid D) \\propto \\pi(D \\mid q) \\pi(q)\\)\nHere, q represents all the unknown quantities we are trying to learn. These could be parameters like growth rates, optimal temperatures, or bite rate. The space of all possible values that q could take is what we call the parameter space. In simple models, this space might be one or two dimensional. In more realistic models—like those describing mosquito traits and parasite development—it can easily become 10, 20, or even hundreds of dimensions.\nNow, suppose we want to estimate something from this posterior—say, the average value of one trait or the probability that it exceeds a certain threshold. In Bayesian statistics, this means we need to compute expectations, which take the form:\n\\(\\mathbb{E}_{\\pi}[f] = \\int dq \\, \\pi(q \\mid D) f(q)\\)\nThis is the central computation in Bayesian inference. But computing such integrals extremely hard in high dimensions because it does not just evaluate the integrand, it aggregates it over volumns.\nImagine covering a 1D space (a line) with intervals.\n\n\n\n\n\nIn 2D, covering a square requires 9 tiles—one in the center and 8 around.\n\n\n\n\n\nIn 3D, a cube is surrounded by 26 others.\n\n\n\n\n\nAs dimensions increase, this “surrounding” effect grows rapidly. The volume of the space explodes outward, and the small region near the peak of our distribution (where we expect to learn the most) becomes a tiny fraction of the whole space.\nThis is a core problem. Even if we know where the posterior is highest (its “peak”), most of the probability mass lies in a weird, thin shell around it, not directly at the center. And trying to integrate over this complex shape with regular grid-based methods (like the ones we learn in calculus) just isn’t feasible when the dimension goes beyond 3 or 4.\nSo, to actually compute expectations in high-dimensional parameter spaces, we need smarter tools. This is where Monte Carlo method step in, and eventually, Markov Chain Monte Carlo come into play."
  },
  {
    "objectID": "mcmc.html#monte-carlo-sampling-instead-of-solving",
    "href": "mcmc.html#monte-carlo-sampling-instead-of-solving",
    "title": "MCMC Tutorial",
    "section": "Monte Carlo: Sampling Instead of Solving",
    "text": "Monte Carlo: Sampling Instead of Solving\nSo far, we’ve learned that Bayesian inference boils down to computing an expectation: an integral over a high-dimensional parameter space. But as we saw, volume in high dimensions behaves oddly—making that integral nearly impossible to solve with traditional methods like grid search or numeric quadrature. The regions that matter most are small and hard to find.\nThis is where Monte Carlo methods step in. Rather than trying to compute the integral directly, Monte Carlo flips the problem: it draws random samples from the posterior distribution, and then approximates the expectation by averaging over those samples.\nThink of it like this: imagine trying to understand how students perform in a large university, but instead of surveying every single student, you randomly sample students from the most populated classes—because that’s where most of the action happens. Similarly, Monte Carlo sampling draws points from areas where the posterior distribution places the most weight. These areas, known as the typical set, are where most of the useful information lives. So instead of wasting effort sampling from the outskirts, Monte Carlo methods focus where it counts.\nMathematically, if we draw N independent samples \\(q_1, q_2, \\ldots, q_N\\) from the posterior \\(\\pi(q \\mid D)\\), the Monte Carlo estimator is simply:\n\\[\\frac{1}{N} \\sum_{n=1}^{N} f(q_n) \\sim \\mathcal{N}\\left(\\mathbb{E}[f], \\frac{\\mathrm{Var}[f]}{N} \\right)\\]\nBut there’s a challenge.\nSampling from a complex posterior is not straightforward. In order to draw the sample, we need to know where the typical set is. They are the same problem. All I have done is recasting the problem in a different manner. The typical set is a thin, hard-to-find slice of the full space. If our samples miss it, then the estimate is wrong. Monte Carlo tells us what to do, but not how to do it.\nAnd that’s where Markov chains come in."
  },
  {
    "objectID": "mcmc.html#markov-chains-stepping-through-the-posterior",
    "href": "mcmc.html#markov-chains-stepping-through-the-posterior",
    "title": "MCMC Tutorial",
    "section": "Markov Chains: Stepping Through the Posterior",
    "text": "Markov Chains: Stepping Through the Posterior\nInstead of sampling independently, we construct a sequence of correlated samples. We start at one point \\(q{\\prime}\\) in parameter space and generate the next point q using a transition rule, or what’s formally called a Markov transition kernel, written as:\n\\[T(q \\mid q{\\prime})\\]\nThis transition rule defines the probability of jumping from one state to another. We repeat this process—step by step—generating a chain of points that slowly “wanders” through the parameter space. You can think of it as taking a random walk.\nThis alone doesn’t help much. But here’s the key insight: if we engineer this transition kernel \\(T(q \\mid q{\\prime})\\) to preserve the posterior distribution, then something powerful happens.\nFormally, the stationary distribution of the chain becomes our target posterior:\n\\[\\pi(q) = \\int \\pi(q{\\prime}) \\, T(q \\mid q{\\prime}) \\, dq{\\prime}\\]\nThis equation says: if we draw enough samples using a well-designed Markov transition, the resulting distribution of the chain will match the posterior we care about. Even though the individual samples are correlated, they still reflect the true structure of the target distribution.\n\n\n\nMarkov transition preserves target distribution that natually concentrates towards the typical set\n\n\nSo, Markov chains let us do the impossible: sample from complex posteriors without needing a full map of the space. Instead, we let the chain guide us, one probabilistic step at a time.\n\nWe combine this idea with Monte Carlo, and it leads us to the powerhouse technique known as Markov Chain Monte Carlo (MCMC)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Final Project",
    "section": "",
    "text": "In this project, we aim to recreate a specific component of the analysis from Johnson et al. (2015), which investigates how temperature influences the transmission potential of malaria through its effect on various mosquito and parasite traits. Rather than reproducing the full reproductive number \\(R_0\\), we focus on a single temperature-sensitive trait — parasite development rate (PDR) and mosquito mortality — to explore how Bayesian inference can quantify uncertainty in that trait’s temperature response. The data for this analysis come from the VectorByte platform, which provides curated temperature-dependent trait data relevant to vector-borne diseases. By implementing and visualizing this subset of the model, we illustrate the impact of trait uncertainty on broader disease transmission dynamics, using methods and approaches aligned with those described in the original paper."
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "conclusions.html#quarto",
    "href": "conclusions.html#quarto",
    "title": "Conclusions",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "conclusions.html#running-code",
    "href": "conclusions.html#running-code",
    "title": "Conclusions",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "introduction.html#likelihood-definition",
    "href": "introduction.html#likelihood-definition",
    "title": "Final Project",
    "section": "2.1 Likelihood Definition",
    "text": "2.1 Likelihood Definition\nWe will use the parasite deveolpment rate (PDR) as an illustration of the general workflow. According to the paper, researchers assumed that trait values vary nonlinearly with temperature and exhibit a unimodal, asymmetric pattern. This biological response was modeled using a Brière function, a common choice for temperature-dependent traits that increase sharply, peak at an intermediate temperature, and decline at high temperatures:\n\\[f(T) = c \\cdot T \\cdot (T - T_0) \\cdot \\sqrt{T_m - T}, \\quad \\text{for } T_0 &lt; T &lt; T_m\\], where\n\nT is the temperature (°C),\nT_0 is the lower developmental threshold,\nT_m is the upper thermal limit,\nc is a scaling constant."
  }
]